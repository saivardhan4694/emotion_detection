{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    "    )\n",
    "validation_data_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    \"/mnt/d/repositories/emotion_detection/data/raw/train\",\n",
    "    target_size=(48,48),\n",
    "    batch_size= 64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_data_gen.flow_from_directory(\n",
    "    \"/mnt/d/repositories/emotion_detection/data/raw/test\",\n",
    "    target_size=(48,48),\n",
    "    batch_size= 64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 11:35:59.251096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-12 11:35:59.407206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-12 11:35:59.407266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-12 11:35:59.411674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-12 11:35:59.411736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-12 11:35:59.411761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-12 11:35:59.973818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-12 11:35:59.973947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-12 11:35:59.973966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-10-12 11:35:59.974000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-12 11:35:59.974103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "emotion_model = Sequential()\n",
    "\n",
    "# First Convolutional Block\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(48, 48, 1)))\n",
    "emotion_model.add(BatchNormalization()) \n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "# Second Convolutional Block\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3)))\n",
    "emotion_model.add(BatchNormalization()) \n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "# Third Convolutional Block\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3)))\n",
    "emotion_model.add(BatchNormalization())\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "# Fully Connected Layer\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(256, activation='relu')) \n",
    "emotion_model.add(BatchNormalization()) \n",
    "emotion_model.add(Dropout(0.5))\n",
    "\n",
    "# Output Layer\n",
    "emotion_model.add(Dense(7, activation='softmax')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 46, 46, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 46, 46, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 23, 23, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 23, 23, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 21, 21, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 21, 21, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 10, 10, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10, 10, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 8, 8, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               524544    \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 620935 (2.37 MB)\n",
      "Trainable params: 619975 (2.37 MB)\n",
      "Non-trainable params: 960 (3.75 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emotion_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 11:42:10.944641: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2024-10-12 11:42:11.382889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8904\n",
      "2024-10-12 11:42:12.180656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-10-12 11:42:12.440847: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f3058117390 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-10-12 11:42:12.440919: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2024-10-12 11:42:12.517464: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-12 11:42:12.810317: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - ETA: 0s - loss: 2.1321 - accuracy: 0.2182Best model saved at epoch 1 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_epoch_1.h5\n",
      "Model structure saved at epoch 1 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_structure_epoch_1.json\n",
      "448/448 [==============================] - 107s 229ms/step - loss: 2.1321 - accuracy: 0.2182 - val_loss: 1.8245 - val_accuracy: 0.2493\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saivardhan4694/miniconda3/envs/myenv/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 99s 221ms/step - loss: 1.8193 - accuracy: 0.2640 - val_loss: 1.8555 - val_accuracy: 0.2973\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 94s 211ms/step - loss: 1.7578 - accuracy: 0.2863 - val_loss: 1.9553 - val_accuracy: 0.3117\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.7318 - accuracy: 0.2996Best model saved at epoch 4 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_epoch_4.h5\n",
      "Model structure saved at epoch 4 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_structure_epoch_4.json\n",
      "448/448 [==============================] - 97s 216ms/step - loss: 1.7318 - accuracy: 0.2996 - val_loss: 1.6171 - val_accuracy: 0.3905\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 99s 222ms/step - loss: 1.7002 - accuracy: 0.3216 - val_loss: 1.6793 - val_accuracy: 0.3491\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.6770 - accuracy: 0.3300Best model saved at epoch 6 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_epoch_6.h5\n",
      "Model structure saved at epoch 6 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_structure_epoch_6.json\n",
      "448/448 [==============================] - 119s 266ms/step - loss: 1.6770 - accuracy: 0.3300 - val_loss: 1.5427 - val_accuracy: 0.3982\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.6599 - accuracy: 0.3396Best model saved at epoch 7 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_epoch_7.h5\n",
      "Model structure saved at epoch 7 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_structure_epoch_7.json\n",
      "448/448 [==============================] - 117s 262ms/step - loss: 1.6599 - accuracy: 0.3396 - val_loss: 1.5151 - val_accuracy: 0.4188\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.6431 - accuracy: 0.3492Best model saved at epoch 8 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_epoch_8.h5\n",
      "Model structure saved at epoch 8 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_structure_epoch_8.json\n",
      "448/448 [==============================] - 117s 260ms/step - loss: 1.6431 - accuracy: 0.3492 - val_loss: 1.4806 - val_accuracy: 0.4259\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 116s 258ms/step - loss: 1.6215 - accuracy: 0.3630 - val_loss: 1.5929 - val_accuracy: 0.3956\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 118s 262ms/step - loss: 1.6091 - accuracy: 0.3667 - val_loss: 1.7939 - val_accuracy: 0.3644\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 117s 262ms/step - loss: 1.6020 - accuracy: 0.3740 - val_loss: 6.3924 - val_accuracy: 0.2875\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.5877 - accuracy: 0.3767Best model saved at epoch 12 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_epoch_12.h5\n",
      "Model structure saved at epoch 12 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_structure_epoch_12.json\n",
      "448/448 [==============================] - 124s 276ms/step - loss: 1.5877 - accuracy: 0.3767 - val_loss: 1.4598 - val_accuracy: 0.4457\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.5688 - accuracy: 0.3861Best model saved at epoch 13 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_epoch_13.h5\n",
      "Model structure saved at epoch 13 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_structure_epoch_13.json\n",
      "448/448 [==============================] - 126s 280ms/step - loss: 1.5688 - accuracy: 0.3861 - val_loss: 1.4156 - val_accuracy: 0.4577\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 117s 262ms/step - loss: 1.5634 - accuracy: 0.3910 - val_loss: 1.5708 - val_accuracy: 0.4329\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.5561 - accuracy: 0.3947Best model saved at epoch 15 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_epoch_15.h5\n",
      "Model structure saved at epoch 15 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_structure_epoch_15.json\n",
      "448/448 [==============================] - 158s 354ms/step - loss: 1.5561 - accuracy: 0.3947 - val_loss: 1.3773 - val_accuracy: 0.4805\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 120s 268ms/step - loss: 1.5501 - accuracy: 0.3963 - val_loss: 1.3848 - val_accuracy: 0.4780\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 119s 266ms/step - loss: 1.5408 - accuracy: 0.3989 - val_loss: 1.4432 - val_accuracy: 0.4669\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.5321 - accuracy: 0.4045Best model saved at epoch 18 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_epoch_18.h5\n",
      "Model structure saved at epoch 18 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_structure_epoch_18.json\n",
      "448/448 [==============================] - 117s 262ms/step - loss: 1.5321 - accuracy: 0.4045 - val_loss: 1.3365 - val_accuracy: 0.5035\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 116s 260ms/step - loss: 1.5260 - accuracy: 0.4128 - val_loss: 1.4644 - val_accuracy: 0.4551\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 116s 259ms/step - loss: 1.5171 - accuracy: 0.4142 - val_loss: 2.9151 - val_accuracy: 0.3167\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 117s 260ms/step - loss: 1.5130 - accuracy: 0.4131 - val_loss: 1.3850 - val_accuracy: 0.4756\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.5143 - accuracy: 0.4132Best model saved at epoch 22 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_epoch_22.h5\n",
      "Model structure saved at epoch 22 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_structure_epoch_22.json\n",
      "448/448 [==============================] - 123s 275ms/step - loss: 1.5143 - accuracy: 0.4132 - val_loss: 1.3047 - val_accuracy: 0.5025\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 121s 270ms/step - loss: 1.5049 - accuracy: 0.4182 - val_loss: 1.3218 - val_accuracy: 0.5107\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.4922 - accuracy: 0.4194Best model saved at epoch 24 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_epoch_24.h5\n",
      "Model structure saved at epoch 24 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_structure_epoch_24.json\n",
      "448/448 [==============================] - 122s 272ms/step - loss: 1.4922 - accuracy: 0.4194 - val_loss: 1.2791 - val_accuracy: 0.5219\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 122s 271ms/step - loss: 1.4923 - accuracy: 0.4205 - val_loss: 1.3598 - val_accuracy: 0.5081\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 116s 259ms/step - loss: 1.4889 - accuracy: 0.4267 - val_loss: 1.7748 - val_accuracy: 0.4640\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 119s 265ms/step - loss: 1.4891 - accuracy: 0.4255 - val_loss: 1.3003 - val_accuracy: 0.5124\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 117s 261ms/step - loss: 1.4764 - accuracy: 0.4332 - val_loss: 1.3488 - val_accuracy: 0.4891\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 118s 262ms/step - loss: 1.5088 - accuracy: 0.4161 - val_loss: 1.2944 - val_accuracy: 0.5107\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "import json\n",
    "\n",
    "class BestModelCheckpointEveryNEpochs(Callback):\n",
    "    def __init__(self, save_path_template, save_structure_path_template):\n",
    "        super().__init__()\n",
    "        self.save_path_template = save_path_template\n",
    "        self.save_structure_path_template = save_structure_path_template\n",
    "        self.best_val_loss = float('inf')  # Initialize to a large number\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current_val_loss = logs.get('val_loss')  # Get the current validation loss\n",
    "        if current_val_loss is not None:\n",
    "            # Check if the current model is the best one so far\n",
    "            if current_val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = current_val_loss\n",
    "                \n",
    "                # Save the model\n",
    "                filename = self.save_path_template.format(epoch=epoch + 1)\n",
    "                self.model.save(filename)\n",
    "                print(f'Best model saved at epoch {epoch + 1} to {filename}')\n",
    "                \n",
    "                # Save the model architecture as JSON\n",
    "                model_json = self.model.to_json()  # Get model architecture as JSON\n",
    "                structure_filename = self.save_structure_path_template.format(epoch=epoch + 1)\n",
    "                with open(structure_filename, 'w') as json_file:\n",
    "                    json_file.write(model_json)\n",
    "                print(f'Model structure saved at epoch {epoch + 1} to {structure_filename}')\n",
    "\n",
    "# Define the early stopping and model checkpoint callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# File path templates for saving the best model and its structure in the specified directory\n",
    "save_path_template = '/mnt/d/repositories/emotion_detection/models/best_emotion_model_epoch_{epoch}.h5'  # Template for saving model\n",
    "save_structure_path_template = '/mnt/d/repositories/emotion_detection/models/best_emotion_model_structure_epoch_{epoch}.json'  # Template for saving model structure\n",
    "\n",
    "# Instantiate your custom model checkpoint\n",
    "model_checkpoint = BestModelCheckpointEveryNEpochs(save_path_template, save_structure_path_template)\n",
    "\n",
    "# Fit the model with callbacks\n",
    "emotion_model_info = emotion_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=28709 // 64,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=7178 // 64,\n",
    "    callbacks=[early_stop, model_checkpoint]  # Use the custom checkpoint and early stopping\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Althogh the early stopping as triggerd a stop in training, based on my observations on the loss and accuracy values. i have decided to continue tarining since the loss is although fluctuating but still decreasing and accuracy is increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 13:09:38.717451: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 133s 289ms/step - loss: 1.4968 - accuracy: 0.4201 - val_loss: 1.3102 - val_accuracy: 0.5184\n",
      "Epoch 2/20\n",
      "448/448 [==============================] - 1403s 3s/step - loss: 1.4884 - accuracy: 0.4265 - val_loss: 1.2939 - val_accuracy: 0.5092\n",
      "Epoch 3/20\n",
      "448/448 [==============================] - 118s 264ms/step - loss: 1.4769 - accuracy: 0.4303 - val_loss: 1.3274 - val_accuracy: 0.4948\n",
      "Epoch 4/20\n",
      "448/448 [==============================] - ETA: 0s - loss: 1.5013 - accuracy: 0.4217Best model saved at epoch 4 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_epoch_4.h5\n",
      "Model structure saved at epoch 4 to /mnt/d/repositories/emotion_detection/models/best_emotion_model_structure_epoch_4.json\n",
      "448/448 [==============================] - 118s 262ms/step - loss: 1.5013 - accuracy: 0.4217 - val_loss: 1.2727 - val_accuracy: 0.5166\n",
      "Epoch 5/20\n",
      "448/448 [==============================] - 117s 262ms/step - loss: 1.4722 - accuracy: 0.4321 - val_loss: 1.3493 - val_accuracy: 0.5003\n",
      "Epoch 6/20\n",
      "448/448 [==============================] - 117s 261ms/step - loss: 1.4831 - accuracy: 0.4287 - val_loss: 1.3744 - val_accuracy: 0.5093\n",
      "Epoch 7/20\n",
      "448/448 [==============================] - 117s 260ms/step - loss: 1.4745 - accuracy: 0.4305 - val_loss: 1.3403 - val_accuracy: 0.5066\n",
      "Epoch 8/20\n",
      "448/448 [==============================] - 117s 261ms/step - loss: 1.4667 - accuracy: 0.4332 - val_loss: 1.3012 - val_accuracy: 0.5174\n",
      "Epoch 9/20\n",
      "448/448 [==============================] - 137s 306ms/step - loss: 1.4626 - accuracy: 0.4360 - val_loss: 1.2896 - val_accuracy: 0.5212\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the last saved model\n",
    "model = load_model('/mnt/d/repositories/emotion_detection/models/best_emotion_model_epoch_24.h5')\n",
    "\n",
    "# Set up the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "emotion_model_info = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=28709 // 64,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=7178 // 64,\n",
    "    callbacks=[early_stop, model_checkpoint]  # Use the custom checkpoint and early stopping\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
